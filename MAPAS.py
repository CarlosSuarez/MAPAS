# app.py ‚Äî PCA + Clustering + Clasificaci√≥n (presentaci√≥n Streamlit, versi√≥n con Caracterizaci√≥n PC)
import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path

from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import lightgbm as lgb

# ================== Config general & estilo ==================
st.set_page_config(page_title="Fenotipos Hemodin√°micos (PCA)", page_icon="üß¨", layout="wide")
st.title("üß¨ Fenotipos Hemodin√°micos con PCA: Clustering y Clasificaci√≥n")

# Tabs fijas (sticky) al hacer scroll
STICKY_TABS_CSS = """
<style>
.stTabs [role="tablist"]{
  position: sticky; top: 0; z-index: 999;
  background: var(--background-color);
  padding-top: .5rem; margin-top: -0.5rem;
  border-bottom: 1px solid rgba(255,255,255,0.1);
}
.block-container { padding-top: 0.5rem; }
</style>
"""
st.markdown(STICKY_TABS_CSS, unsafe_allow_html=True)

# ================== Sidebar: Fuente de datos y par√°metros ==================
st.sidebar.header("‚öôÔ∏è Datos & Par√°metros")

DATA_SOURCE = st.sidebar.radio(
    "Fuente de datos (componentes PCA)",
    ["Auto (archivo local)", "URL (GitHub/raw)", "Subir CSV"],
    index=0,
)
DEFAULT_PATHS = [Path("datos_final7_pca_components.csv"), Path("data/datos_final7_pca_components.csv")]

@st.cache_data(show_spinner=False)
def load_csv_from_path(p: Path) -> pd.DataFrame:
    return pd.read_csv(p)

@st.cache_data(show_spinner=False)
def load_csv_from_url(url: str) -> pd.DataFrame:
    return pd.read_csv(url)

@st.cache_data(show_spinner=False)
def load_csv_from_upload(file) -> pd.DataFrame:
    return pd.read_csv(file)

df = None
used_source_desc = ""

if DATA_SOURCE == "Auto (archivo local)":
    found = next((p for p in DEFAULT_PATHS if p.exists()), None)
    if found is not None:
        df = load_csv_from_path(found)
        used_source_desc = f"üìÅ Cargado autom√°ticamente desde `{found}`"
    else:
        st.warning("No se encontr√≥ `datos_final7_pca_components.csv` (ni en `./` ni en `./data/`). "
                   "Selecciona **URL** o **Subir CSV** en la barra lateral.")
elif DATA_SOURCE == "URL (GitHub/raw)":
    url_input = st.sidebar.text_input(
        "URL directa al CSV (raw de GitHub u otro servidor)",
        value="",
        placeholder="https://raw.githubusercontent.com/usuario/repo/rama/data/datos_final7_pca_components.csv",
    )
    if url_input:
        try:
            df = load_csv_from_url(url_input)
            used_source_desc = "üîó Cargado desde URL"
        except Exception as e:
            st.error(f"No se pudo leer la URL. Detalle: {e}")
else:
    uploaded_file = st.sidebar.file_uploader("Sube tu CSV (componentes PCA)", type=["csv"])
    if uploaded_file:
        df = load_csv_from_upload(uploaded_file)
        used_source_desc = "‚¨ÜÔ∏è Cargado por subida de archivo"

if df is None:
    st.stop()

st.success(f"‚úÖ Datos cargados. {used_source_desc}")

# ======== Detecci√≥n de columnas PCA y elecci√≥n de PCs a usar ========
def detectar_pc_cols(_df: pd.DataFrame):
    pc_named = [c for c in _df.columns if str(c).upper().startswith("PC")]
    if pc_named:
        def pc_order(c):
            s = ''.join(ch for ch in str(c) if ch.isdigit())
            return int(s) if s.isdigit() else 9999
        return sorted(pc_named, key=pc_order)
    return [c for c in _df.columns if pd.api.types.is_numeric_dtype(_df[c])]

pc_cols_all = detectar_pc_cols(df)
if len(pc_cols_all) < 2:
    st.error("No se detectaron suficientes columnas de componentes principales (PCs).")
    st.stop()

default_pcs = [c for c in pc_cols_all if any(c.upper() == f"PC{i}" for i in range(1, 6))]
if not default_pcs:
    default_pcs = pc_cols_all[:5] if len(pc_cols_all) >= 5 else pc_cols_all

pcs_to_use = st.sidebar.multiselect(
    "PCs a usar para clustering y modelos",
    options=pc_cols_all,
    default=default_pcs,
)
if len(pcs_to_use) < 2:
    st.warning("Selecciona al menos 2 componentes para proceder.")
    st.stop()

# Par√°metros de clustering/modelado
k_optimo = st.sidebar.slider("N√∫mero de Clusters (k)", 2, 10, 4)
test_size = st.sidebar.slider("Proporci√≥n Test", 0.1, 0.5, 0.25, step=0.05)
random_state = 42

# ======== Preparaci√≥n base ========
df_pca = df.copy()
df_pca = df_pca.dropna(subset=pcs_to_use)  # PCA suele venir limpio
X_pca_all = df_pca[pc_cols_all].values
X_pca = df_pca[pcs_to_use].values

# ================== NUEVO M√ìDULO 1: Caracterizaci√≥n PC ==================
# Matriz de loadings tomada de tu gr√°fico (PC1..PC8 x variables hemodin√°micas)
LOADINGS_DEFAULT = pd.DataFrame(
    data=[
        [-0.10, -0.03,  0.49,  0.19,  0.49,  0.30,  0.43,  0.45],  # PC1
        [ 0.53,  0.50,  0.06,  0.46,  0.06,  0.40, -0.25, -0.20],  # PC2
        [ 0.44,  0.53, -0.03, -0.47,  0.02, -0.37,  0.30,  0.29],  # PC3
        [-0.10,  0.05, -0.50, -0.45,  0.41,  0.51, -0.27,  0.17],  # PC4
        [ 0.66, -0.64, -0.10,  0.07,  0.13, -0.13, -0.16,  0.27],  # PC5
        [-0.27,  0.24, -0.16,  0.40,  0.16, -0.42, -0.46,  0.52],  # PC6
        [-0.00,  0.00, -0.40,  0.24,  0.60, -0.32,  0.35, -0.45],  # PC7
        [ 0.00, -0.00,  0.56, -0.34,  0.43, -0.23, -0.49, -0.32],  # PC8
    ],
    index=[f"PC{i}" for i in range(1, 9)],
    columns=[
        "fc_mean","fc_night_mean","sbp_mean","dbp_mean",
        "sbp_night_mean","dbp_night_mean","pulse_pressure_mean","pulse_pressure_night_mean"
    ],
)

# (Opcional) permitir cargar un CSV de loadings para sustituir el default
with st.expander("üß© (Opcional) Cargar CSV de loadings para el heatmap", expanded=False):
    up_load = st.file_uploader("CSV con matriz de loadings (filas=PCs, columnas=variables)", type=["csv"], key="up_loadings")
    if up_load is not None:
        try:
            tmp = pd.read_csv(up_load, index_col=0)
            if tmp.shape[0] >= 2 and tmp.shape[1] >= 2:
                LOADINGS_DEFAULT = tmp
                st.success("Loadings cargados desde tu archivo.")
            else:
                st.error("El CSV debe tener al menos 2 filas (PCs) y 2 columnas (variables).")
        except Exception as e:
            st.error(f"No se pudo leer el CSV de loadings: {e}")

# ================== Tabs (orden solicitado) ==================
tabA, tab0, tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üß© Caracterizaci√≥n PC",
    "üëÄ Datos",
    "üìä Diagn√≥stico & Clustering",
    "üîé Exploraci√≥n en PCA",
    "ü§ñ Modelado (sobre PCs)",
    "üìà Evaluaci√≥n",
    "üèÜ Conclusiones",
])

# ============ TAB A: Caracterizaci√≥n PC ============
with tabA:
    st.subheader("üß© PCA Loadings Heatmap (excluyendo edad, IMC y g√©nero)")
    fig, ax = plt.subplots(figsize=(8, 5))
    sns.heatmap(
        LOADINGS_DEFAULT, annot=True, fmt=".2f", cmap="viridis",
        vmin=-0.7, vmax=0.7, cbar_kws={"label":"Loading"}, ax=ax
    )
    ax.set_xlabel("Variables originales"); ax.set_ylabel("Componentes principales")
    ax.set_title("PCA Loadings Heatmap")
    st.pyplot(fig)
    st.caption("Los *loadings* indican la contribuci√≥n (y signo) de cada variable original a cada componente principal.")

    st.markdown("""
### Caracterizaci√≥n de los componentes principales (PC1‚ÄìPC5)

- **PC1 ‚Äî Presi√≥n de Pulso y Sist√≥lica (PP/SBP ‚Üë):** cargas positivas altas en *pulse_pressure_mean*, *pulse_pressure_night_mean*, *sbp_mean*, *sbp_night_mean*.  
  ‚Üí Valores altos = hipertensi√≥n **sist√≥lica** y **pulso** elevados.

- **PC2 ‚Äî Presi√≥n Diast√≥lica y Frecuencia Card√≠aca (DBP/FC ‚Üë):** cargas positivas altas en *dbp_mean*, *dbp_night_mean*, *fc_mean*, *fc_night_mean*.  
  ‚Üí Valores altos = **diast√≥lica** y **frecuencia** elevadas (taquicardia).

- **PC3 ‚Äî Frecuencia vs. Pulso (contraste FC ‚Üë ‚Üî PP ‚Üì):** positivo en FC; negativo en PP.  
  ‚Üí Altos PC3 = **FC alta** con **PP baja**; bajos PC3 = **PP alta** con **FC controlada**.

- **PC4 ‚Äî Patr√≥n Nocturno Anormal (No Dipper):** positivo en presiones nocturnas y negativo en diurnas.  
  ‚Üí Altos PC4 = patr√≥n **Non-Dipper/Riser** (mayor presi√≥n nocturna).

- **PC5 ‚Äî Patr√≥n de Pulso Diurno (Dipper fuerte):** positivo en PP/SBP diurnas y negativo en nocturnas.  
  ‚Üí Altos PC5 = **Dipper** pronunciado (descenso nocturno saludable).
""")

# ============ TAB 0: Datos ============
with tab0:
    st.subheader("üëÄ Vista previa de los datos (componentes PCA)")
    c1, c2, c3 = st.columns(3)
    with c1: st.metric("Filas", f"{df_pca.shape[0]:,}")
    with c2: st.metric("Columnas", f"{df_pca.shape[1]:,}")
    with c3: st.metric("PCs detectados", f"{len(pc_cols_all)}")
    st.caption(f"PCs seleccionados para el flujo: {pcs_to_use}")
    st.dataframe(df_pca.head(50), use_container_width=True)

# ============ TAB 1: Diagn√≥stico & Clustering ============
with tab1:
    st.subheader("üìä M√©todo del Codo (en espacio PCA)")
    inertia = []
    k_values = range(2, 11)
    for k in k_values:
        km = KMeans(n_clusters=k, random_state=random_state, n_init=10)
        km.fit(X_pca_all)  # codo con TODOS los PCs detectados
        inertia.append(km.inertia_)

    fig, ax = plt.subplots(figsize=(6, 3.5))
    ax.plot(list(k_values), inertia, marker="o")
    ax.set_xlabel("N√∫mero de Clusters (k)"); ax.set_ylabel("Inercia"); ax.grid(True, alpha=.3)
    st.pyplot(fig)

    # Clustering final con PCs seleccionados
    kmeans_final = KMeans(n_clusters=k_optimo, random_state=random_state, n_init=50)
    df_pca["fenotipo"] = kmeans_final.fit_predict(X_pca)

    dist_df = df_pca["fenotipo"].value_counts().rename_axis("fenotipo").reset_index(name="count")
    st.success(f"‚úÖ Se han descubierto {k_optimo} fenotipos")
    st.dataframe(dist_df.sort_values("fenotipo").reset_index(drop=True), use_container_width=True)

    # (Se retir√≥ la gr√°fica de ‚ÄúFenotipos mapeados‚Äù como solicitaste)

# ============ TAB 2: Exploraci√≥n en PCA ============
with tab2:
    st.subheader("üîé Exploraci√≥n en el espacio PCA")

    # 1) Scatter PC1 vs PC2 por fenotipo (√∫til, lo mantenemos)
    pcs = detectar_pc_cols(df_pca)
    PC1 = next((c for c in pcs if c.upper()=="PC1"), pcs[0])
    PC2 = next((c for c in pcs if c.upper()=="PC2"), pcs[1] if len(pcs)>1 else pcs[0])

    fig, ax = plt.subplots(figsize=(7, 4.8))
    sns.scatterplot(data=df_pca, x=PC1, y=PC2, hue="fenotipo", palette="viridis", s=18, alpha=0.8, ax=ax)
    ax.set_title("Fenotipos en PC1 vs PC2"); ax.grid(True, alpha=.3)
    st.pyplot(fig)

    # 2) ‚ÄúComportamiento de Fenotipos en el Espacio PCA‚Äù (pairplot del c√≥digo original)
    st.markdown("**Comportamiento de Fenotipos en el Espacio PCA (pairplot)**")
    # Para evitar plots muy pesados, muestreamos hasta N filas
    max_pts = st.slider("M√°x. puntos para pairplot (para rendimiento)", 500, 5000, 2000, 500)
    df_pair = df_pca.copy()
    if len(df_pair) > max_pts:
        df_pair = df_pair.sample(max_pts, random_state=42)

    vars_pair = [c for c in pc_cols_all if any(c.upper() == f"PC{i}" for i in range(1,6))]
    if not vars_pair:
        vars_pair = pc_cols_all[:5]

    pair = sns.pairplot(
        df_pair,
        hue="fenotipo",
        palette="viridis",
        vars=vars_pair,
        plot_kws=dict(s=12, alpha=0.7),
        diag_kws=dict(fill=True, alpha=0.7),
    )
    pair.fig.suptitle('Comportamiento de Fenotipos en el Espacio PCA', y=1.02)
    st.pyplot(pair.fig)

# ============ TAB 3: Modelado (sobre PCs) ============
with tab3:
    st.subheader("ü§ñ Modelado supervisado usando PCs seleccionados")
    X = df_pca[pcs_to_use]
    y = df_pca["fenotipo"]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )

    classifiers = {
        "LogisticRegression": (LogisticRegression(max_iter=1000), {'C': [0.1, 1, 10]}),
        "KNN": (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),
        "SVC": (SVC(), {'C': [1, 10], 'kernel': ['rbf']}),
        "DecisionTree": (DecisionTreeClassifier(random_state=42), {'max_depth': [5, 10, None]}),
        "RandomForest": (RandomForestClassifier(random_state=42), {'n_estimators': [100, 200]}),
        "GradientBoosting": (GradientBoostingClassifier(random_state=42), {'n_estimators': [100]}),
        "LightGBM": (lgb.LGBMClassifier(random_state=42), {'n_estimators': [100]}),
        "GaussianNB": (GaussianNB(), {}),  # sin hiperpar√°metros
    }

    best_models = {}
    rows = []
    for name, (model, params) in classifiers.items():
        with st.spinner(f"Entrenando {name}..."):
            grid = GridSearchCV(model, params, cv=3, scoring="f1_macro", n_jobs=-1)
            grid.fit(X_train, y_train)
            best_models[name] = grid.best_estimator_
            rows.append({
                "Modelo": name,
                "F1 (CV)": grid.best_score_,
                "Mejores par√°metros": str(grid.best_params_)
            })

    results_cv_df = pd.DataFrame(rows).sort_values("F1 (CV)", ascending=False).reset_index(drop=True)
    st.markdown("**Resultados (validaci√≥n cruzada)**")
    st.dataframe(results_cv_df.style.format({"F1 (CV)": "{:.3f}"}), use_container_width=True)

    st.session_state["split"] = (X_train, X_test, y_train, y_test)
    st.session_state["best_models"] = best_models

# ============ TAB 4: Evaluaci√≥n ============
with tab4:
    st.subheader("üìà Evaluaci√≥n en test")
    if "best_models" not in st.session_state:
        st.info("Primero entrena los modelos en la pesta√±a **Modelado (sobre PCs)**.")
    else:
        X_train, X_test, y_train, y_test = st.session_state["split"]
        best_models = st.session_state["best_models"]

        results = []
        for name, model in best_models.items():
            y_pred = model.predict(X_test)
            acc = accuracy_score(y_test, y_pred)
            f1 = classification_report(y_test, y_pred, output_dict=True)["macro avg"]["f1-score"]
            results.append({"Modelo": name, "Accuracy (test)": acc, "F1-Macro (test)": f1})

        results_test_df = pd.DataFrame(results).sort_values("F1-Macro (test)", ascending=False).reset_index(drop=True)
        st.markdown("**Ranking en Test**")
        st.dataframe(results_test_df.style.format({"Accuracy (test)": "{:.3f}", "F1-Macro (test)": "{:.3f}"}), use_container_width=True)

        st.markdown("---")
        st.markdown("**Matrices de confusi√≥n**")
        for name, model in best_models.items():
            y_pred = model.predict(X_test)
            cm = confusion_matrix(y_test, y_pred)
            fig, ax = plt.subplots(figsize=(4.8, 3.8))
            sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
            ax.set_title(f"{name}")
            ax.set_xlabel("Predicho"); ax.set_ylabel("Real")
            st.pyplot(fig)

# ============ TAB 5: Conclusiones ============
with tab5:
    st.subheader("üèÜ Conclusiones")
    st.markdown("""
**Hallazgos principales**

1) **Separabilidad alta.** El *clustering* en el espacio PCA produce fenotipos bien delimitados. Los modelos supervisados alcanzan **F1-Macro** y **Accuracy** muy altos, lo que indica que los grupos son aprendibles y equilibrados.

2) **Mapeo cl√≠nico interpretable (1..4).** La organizaci√≥n de los fenotipos en PC1/PC2 captura severidad y patr√≥n circadiano, facilitando la interpretaci√≥n cl√≠nica.

3) **Modelos simples triunfan.** El buen desempe√±o de **LogisticRegression** sugiere **separabilidad casi lineal** en el espacio PCA.

**Implicaciones**

- Soporta **estratificaci√≥n de riesgo** y dise√±o de **rutas de manejo** por fenotipo.
- Permite **monitoreo longitudinal** y priorizaci√≥n de variables hemodin√°micas clave.

**Limitaciones & pr√≥ximos pasos**

- Validaci√≥n en cohortes externas y **validaci√≥n temporal** con series longitudinales.
- Analizar **estabilidad del n√∫mero de clusters** (silhouette, gap statistic) y robustez del mapeo si cambian signos/escala.
- A√±adir **explicabilidad** (SHAP/permutaci√≥n) para auditar decisiones sobre PCs.
- Considerar un panel de **loadings** din√°mico (ya incluido) y, si se dispone, visualizar **varianza explicada** por PC.
""")